{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nltk.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idleyui/python-notebook/blob/master/nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yBc4EX9ABgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO0QhgutCGqY",
        "colab_type": "text"
      },
      "source": [
        "## tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lGacfuvCsHA",
        "colab_type": "text"
      },
      "source": [
        "requires punkt sentence tokenizations models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6yET4ZtCSF9",
        "colab_type": "code",
        "outputId": "84f67ba8-6d0e-441d-d5c4-19c42c22f725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBcOhYs2CilG",
        "colab_type": "code",
        "outputId": "b7287311-f343-43f4-a89e-059bf962ec6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_tokenize(\"Dr. Lee, what's your problem? $1.9b\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr.', 'Lee', ',', 'what', \"'s\", 'your', 'problem', '?', '$', '1.9b']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVbGYGfcDCjz",
        "colab_type": "text"
      },
      "source": [
        "another simple regular-expression based tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt-7XOpQC1Qz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRJY9b2zC8vR",
        "colab_type": "code",
        "outputId": "884fb8de-fc2d-449d-c449-0e287fcdbf88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "wordpunct_tokenize(\"Dr. Lee, what's your? $1.9\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr', '.', 'Lee', ',', 'what', \"'\", 's', 'your', '?', '$', '1', '.', '9']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYpfMdVtB8Fy",
        "colab_type": "text"
      },
      "source": [
        "## pos-tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPQm_eGWCAH4",
        "colab_type": "code",
        "outputId": "f6c8ecf9-bb3d-49f8-baf8-42cc4a45f4ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL7e4S_TCD06",
        "colab_type": "code",
        "outputId": "9614d02b-ce37-4238-c395-ccd14020a998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pos_tag(['hello', 'world'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hello', 'NN'), ('world', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKhn-DtbAI0_",
        "colab_type": "text"
      },
      "source": [
        "## stemmer vs lemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evwxDCorHdPF",
        "colab_type": "text"
      },
      "source": [
        "> The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
        ">\n",
        ">However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n",
        ">\n",
        ">Lemmatization and stemming are special cases of normalization. They identify a canonical representative for a set of related word forms.\n",
        ">\n",
        "https://stackoverflow.com/questions/1787110/what-is-the-true-difference-between-lemmatization-vs-stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34FCqHdnA7k-",
        "colab_type": "text"
      },
      "source": [
        "### stemmer\n",
        "[what is the best temming method in python](https://stackoverflow.com/questions/24647400/what-is-the-best-stemming-method-in-python)\n",
        "> Stemmers vary in their aggressiveness. Porter is one of the monst aggressive stemmer for English. I find it usually hurts more than it helps. On the lighter side you can either use a lemmatizer instead as already suggested, or a lighter algorithmic stemmer. The limitation of lemmatizers is that they cannot handle unknown words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR2aHbpfBEAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HckNsuLA_NR",
        "colab_type": "code",
        "outputId": "755a5909-6063-4556-aaad-b7d5ea9a3d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
        "for w in words:\n",
        "    print(w,ps.stem(w))\n",
        "\n",
        "# stem only do matches\n",
        "ps.stem('languages')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "program program\n",
            "programs program\n",
            "programer program\n",
            "programing program\n",
            "programers program\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'languag'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2zy-mU5Dzrn",
        "colab_type": "text"
      },
      "source": [
        "### lemmatizer\n",
        "based on wordnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_UfsO3DARlQ",
        "colab_type": "code",
        "outputId": "807be10b-14ae-4291-ca39-31158fabbe5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemma = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WALC62jeAboZ",
        "colab_type": "code",
        "outputId": "f8193ecf-9860-4128-9b65-eac207ae42c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# default pos is n\n",
        "lemma.lemmatize('is'), lemma.lemmatize('is', pos='v')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('is', 'be')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owQI_sQ8Ad8T",
        "colab_type": "code",
        "outputId": "3b5e8339-2431-48a4-ed03-ad70dec3f4d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lemma.lemmatize('better'), lemma.lemmatize('better', pos='a')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('better', 'good')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO9H6BvWEBNb",
        "colab_type": "text"
      },
      "source": [
        "## working flow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBMwpLWbEStJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# translate treeback pos to wordnet pos\n",
        "# treeback pos: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "# https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
        "\n",
        "# steps: Document->Sentences->Tokens->POS->Lemmas\n",
        "# https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCuUPZ_8AwTx",
        "colab_type": "code",
        "outputId": "26cb7cff-7620-4f45-bbd3-28364adcb770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# lemma from sentence\n",
        "seq = \"What is your problems?\"\n",
        "tokens = word_tokenize(seq)\n",
        "print('tokens:',tokens)\n",
        "wordwithtags = pos_tag(tokens)\n",
        "print('tags:',wordwithtags)\n",
        "for word, tag in wordwithtags:\n",
        "    print(lemma.lemmatize(word, pos=get_wordnet_pos(tag)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokens: ['What', 'is', 'your', 'problems', '?']\n",
            "tags: [('What', 'WP'), ('is', 'VBZ'), ('your', 'PRP$'), ('problems', 'NNS'), ('?', '.')]\n",
            "What\n",
            "be\n",
            "your\n",
            "problem\n",
            "?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD8sEIquEHGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}